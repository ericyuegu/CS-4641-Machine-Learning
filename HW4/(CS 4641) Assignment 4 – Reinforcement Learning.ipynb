{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "***Eric Gu <br>\n",
    "December 3, 2018***\n",
    "\n",
    "- - -\n",
    "\n",
    "This notebook will explore two interesting Markov Decision Process (MDP) problems:\n",
    "* [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/)\n",
    "* [Taxi](https://gym.openai.com/envs/Taxi-v2/) [Dietterich2000]\n",
    "\n",
    "We will explore them using two fundamental methods for solving MDPs:\n",
    "* Value-iteration (VI)\n",
    "* Policy-iteration (PI)\n",
    "\n",
    "Additionally, we will then use a famous reinforcement learning algorithm to solve the same problems: \n",
    "* Q-learning\n",
    "\n",
    "*Written in Jupyter Notebook using OpenAI's Gym [environments](https://gym.openai.com/envs/), and with algorithm implementation borrowed from [Moustafa Alzantot](https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa), [Arthur Juliani](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0), and [Denny Britz](https://github.com/dennybritz/reinforcement-learning/tree/master/DP).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Problems\n",
    "\n",
    "### Frozen Lake\n",
    "Taken from OpenAI's description of [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/):\n",
    "\"The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\" \n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "An episode ends when the agent either reaches the goal or falls in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "Immediately, we should note that although the state space of this stochastic MDP is small (4x4=16), at each step, there is only a 1/3 chance going where you want to go, and a 1/3 chance you go to the left or right instead. This makes the map treacherous, since to successfully navigate to the goal, the agent has to thread a narrow path between 2 holes no matter what. On the upside, the problem is very simple because it has only one goal state. Let's see how our algorithms perform.\n",
    "\n",
    "### Taxi\n",
    "There are four designated locations in the grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends. \n",
    "\n",
    "The grid world looks like the following:\n",
    "```\n",
    "+---------+\n",
    "|R: | : :G|\n",
    "| : : : : |\n",
    "| : : : : |\n",
    "| | : | : |\n",
    "|Y| : |B: |\n",
    "+---------+\n",
    "```\n",
    "\n",
    "##### Actions\n",
    "There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pickup passenger\n",
    "- 5: dropoff passenger\n",
    "\n",
    "##### Rewards\n",
    "There is a reward of -1 for each action and an additional reward of +20 for delivering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
    "\n",
    "Although the grid world appears to be 5x5, there are actually 500 discrete states because the passenger can be in 5 possible locations (including in the taxi) and 4 destination locations.  The Taxi Problem is an example of a hierarchical MDP—a more complex problem involving subgoals and many permutations of destinations & passengers can be abstracted by encoding relevant subproblem information into the state. While this MDP is deterministic, it poses a challenge because of the far larger state space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "From what we know, we expect PI to converge in fewer iterations than VI, and in probably less wall clock time because of the greater computational requirements required by VI. \n",
    "\n",
    "### Value Iteration (VI)\n",
    "We initialize the value functions to arbitrary random values. Let's start by running VI with a variety of values of gamma (discount rate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake environment using Value-Itertion.\n",
    "\"\"\"\n",
    "\n",
    "env_name  = 'FrozenLake-v0'\n",
    "env = gym.make(env_name)\n",
    "env = env.unwrapped\n",
    "\n",
    "def FrozenLakeVI(g = 1.0, n = 100, render = False):\n",
    "    gamma = g\n",
    "\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    start = timer()\n",
    "    optimal_v = value_iteration(env, gamma);\n",
    "    end = timer()\n",
    "\n",
    "    policy = extract_policy(optimal_v, gamma)\n",
    "    policy_score = evaluate_policy(env, policy, gamma, n=n)\n",
    "\n",
    "    print('Policy average score = ', policy_score)\n",
    "    print('Time elapsed: ', end - start)\n",
    "\n",
    "        \n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    max_iterations = 100000\n",
    "    eps = 1e-20\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 1373.\n",
      "Policy average score =  1.0100312304110134e-07\n",
      "Time elapsed:  0.19633457733834803\n",
      "Value-iteration converged at iteration# 1373.\n",
      "Policy average score =  0.00043486333365813445\n",
      "Time elapsed:  0.19288121571753436\n",
      "Value-iteration converged at iteration# 1373.\n",
      "Policy average score =  0.07167804518900966\n",
      "Time elapsed:  0.19060545382308192\n",
      "Value-iteration converged at iteration# 1373.\n",
      "Policy average score =  0.18807430252169385\n",
      "Time elapsed:  0.1956689774933693\n",
      "Value-iteration converged at iteration# 1373.\n",
      "Policy average score =  0.5598022541207036\n",
      "Time elapsed:  0.19080994347223168\n",
      "Value-iteration converged at iteration# 1373.\n",
      "Policy average score =  0.8184605786228931\n",
      "Time elapsed:  0.19331967379730486\n",
      "Value-iteration converged at iteration# 1373.\n",
      "Policy average score =  0.86\n",
      "Time elapsed:  0.19654558334968897\n"
     ]
    }
   ],
   "source": [
    "for gamma in [0.1, 0.5, 0.9, 0.95, 0.99, 0.999, 1.0]:\n",
    "    FrozenLakeVI(g=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 1373.\n",
      "Policy average score =  0.85\n",
      "Time elapsed:  0.19847070290052216\n"
     ]
    }
   ],
   "source": [
    "FrozenLakeVI(n = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration for Frozen Lake converges at iteration #1373. We can graph the average score for 100 episodes for each value of gamma and find that VI performs best at gamma = 1.0, which makes sense—there is only one goal state, with no other rewards or punishments that are closer or farther away. The policy extracted from the optimal values has an average score of 0.85 over 1000 episodes, and takes 0.198 seconds to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration (PI)\n",
    "\n",
    "The performance of PI against different values of gamma will be the same as VI's, so we use the same hyperparameters as in VI and compare the resulting scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake environment using Policy iteration.\n",
    "\"\"\"\n",
    "\n",
    "env_name  = 'FrozenLake-v0'\n",
    "env = gym.make(env_name)\n",
    "env = env.unwrapped\n",
    "\n",
    "def FrozenLakePI(g = 1.0, n = 100, render = False):\n",
    "    g = g\n",
    "    \n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    start = timer()\n",
    "    optimal_policy = policy_iteration(env, gamma = g)\n",
    "    end = timer()\n",
    "    \n",
    "    scores = evaluate_policy(env, optimal_policy, gamma = g, n = n)\n",
    "    print('Average scores = ', np.mean(scores))\n",
    "    print('Time elapsed: ', end - start)\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    max_iterations = 200000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 4.\n",
      "Average scores =  0.85\n",
      "Time elapsed:  0.09478203854814637\n"
     ]
    }
   ],
   "source": [
    "FrozenLakePI(n = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too surprising! PI converges in only 4 steps. When evaluating the optimal policy provided by the algorithm, PI has the same average score of 0.85 across 1000 episodes, but only requires 0.0948 seconds to run. Here, the principle advantage over VI is the speed at which the algorithm converges on a solution. PI seems to solve Frozen Lake about twice as quickly as VI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "Given the small state space, we can use a simple table implementation of Q-Learning to store the values for how good it is to take a given action within a given state. In the case of the Frozen Lake environment, we have 4 possible actions (the four directions of movement) for each of 16 possible states, giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.\n",
    "\n",
    "Instead of using an epsilon-greedy or \"epsilon-decreasing\" strategy, we adopt a slightly modified, more robust exploration strategy that adds noise to the greedy selection of the next action from the Q-table in proportion to the size of the Q-values, and which decreases in later iterations as time goes on ([source](https://medium.com/@awjuliani/hi-sung-4f446e95c9b5)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake environment using Q-Learning\n",
    "\"\"\"\n",
    "\n",
    "env_name = 'FrozenLake-v0'\n",
    "env = gym.make(env_name)\n",
    "env = env.unwrapped\n",
    "t_max = 10000\n",
    "\n",
    "def FrozenLakeQLearn_NoisyQ(iter_max = 10000, learning_rate = 0.8, g = 1.0):\n",
    "    # Hyperparameters\n",
    "    num_episodes = iter_max\n",
    "    # Set learning parameters\n",
    "    lr = learning_rate\n",
    "    y = g\n",
    "    \n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    #Initialize table with all zeros\n",
    "    Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    #create lists to contain total rewards and steps per episode\n",
    "    #jList = []\n",
    "    rList = []\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                break\n",
    "        #jList.append(j)\n",
    "        rList.append(rAll)\n",
    "    \n",
    "    end = timer()\n",
    "    \n",
    "    print (\"Score over time: \" +  str(sum(rList)/num_episodes))\n",
    "    print('Time elapsed: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.019\n",
      "Time elapsed:  0.18440715223505322\n",
      "Score over time: 0.03\n",
      "Time elapsed:  0.18318735130651476\n",
      "Score over time: 0.393\n",
      "Time elapsed:  0.6621520689291174\n",
      "Score over time: 0.609\n",
      "Time elapsed:  0.803173902776507\n",
      "Score over time: 0.423\n",
      "Time elapsed:  1.248896174802212\n",
      "Score over time: 0.542\n",
      "Time elapsed:  0.837688280198563\n",
      "Score over time: 0.536\n",
      "Time elapsed:  0.8174633539965726\n"
     ]
    }
   ],
   "source": [
    "for gamma in [0.1, 0.5, 0.9, 0.95, 0.99, 0.999, 1.0]:\n",
    "    FrozenLakeQLearn(iter_max=1000, g=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.183\n",
      "Time elapsed:  0.6339902569307014\n",
      "Score over time: 0.303\n",
      "Time elapsed:  1.2211605430347845\n",
      "Score over time: 0.552\n",
      "Time elapsed:  1.2124137340579182\n",
      "Score over time: 0.536\n",
      "Time elapsed:  1.2240913429995999\n",
      "Score over time: 0.088\n",
      "Time elapsed:  2.419768098043278\n",
      "Score over time: 0.009\n",
      "Time elapsed:  2.8619415450375527\n",
      "Score over time: 0.068\n",
      "Time elapsed:  2.3281614179722965\n",
      "Score over time: 0.019\n",
      "Time elapsed:  3.0015432039508596\n",
      "Score over time: 0.019\n",
      "Time elapsed:  2.4791896189562976\n",
      "Score over time: 0.032\n",
      "Time elapsed:  0.30985289497766644\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.1, 0.5, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99, 0.999, 1.0]:\n",
    "    FrozenLakeQLearn_NoisyQ(iter_max=1000, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the Q-Learning algorithm for 1000 iterations for the following values... \n",
    "* learning rate = [0.1, 0.5, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99, 0.999, 1.0]\n",
    "\n",
    "... we find that the algorithm performs best with a (fixed) learning rate = 0.70 when gamma = 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.0\n",
      "Time elapsed:  0.0007028409745544195\n",
      "Score over time: 0.0\n",
      "Time elapsed:  0.007706664968281984\n",
      "Score over time: 0.0\n",
      "Time elapsed:  0.010750383953563869\n",
      "Score over time: 0.11\n",
      "Time elapsed:  0.0879162399796769\n",
      "Score over time: 0.552\n",
      "Time elapsed:  1.3010737430304289\n",
      "Score over time: 0.637\n",
      "Time elapsed:  14.751405472983606\n",
      "Score over time: 0.63671\n",
      "Time elapsed:  141.6780458620051\n"
     ]
    }
   ],
   "source": [
    "for iterations in [1, 5, 10, 100, 1000, 10000, 100000]:\n",
    "    FrozenLakeQLearn_NoisyQ(iter_max=iterations, g=1.0, learning_rate=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Q-Learning is an online algorithm, it doesn't stop on a definitively optimal solution because it is blind to the MDP's true reward and transition functions. Instead, by running 100,000 iterations, the Q-Learning algorithm has accumulated an average score of 0.63119 by learning approximations of the \"correct policy\" from the history of the rewards it receives by making certain actions in each state. It is apparent that the training time increases proportionately to the number of iterations, but the asymptotic performance shows a learning curve that has plateaued. Running the algorithm for only 10,000 iterations results in an average score of 0.6256, not far from that of 100,000 iterations, but takes <9 seconds compared to the 91 seconds for 100,000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a sanity check, we can try Q-Learning using epsilon-greedy without decreasing epsilon over time. The performance *should* be worse than the above Q-Learning algorithm, where we began with a large epsilon (1.0) and asymptotically decreased it towards zero in order to prioritize exploration early on and exploit what the algorithm learned in later episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake environment using Q-Learning\n",
    "\"\"\"\n",
    "\n",
    "def FrozenLakeQLearn_EpsilonGreedy(iter_max = 10000, learning_rate = 0.8, g = 0.95, epsilon = 0.1):\n",
    "    # Hyperparameters\n",
    "    num_episodes = iter_max\n",
    "    # Set learning parameters\n",
    "    lr = learning_rate\n",
    "    y = g\n",
    "    \n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    #Initialize table with all zeros\n",
    "    Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    #create lists to contain total rewards and steps per episode\n",
    "    #jList = []\n",
    "    rList = []\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                a = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                a = np.argmax(Q[s,:])\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                break\n",
    "        #jList.append(j)\n",
    "        rList.append(rAll)\n",
    "    \n",
    "    end = timer()\n",
    "    \n",
    "    print (\"Score over time: \" +  str(sum(rList)/num_episodes))\n",
    "    print('Time elapsed: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.1215\n",
      "Time elapsed:  4.52724552503787\n"
     ]
    }
   ],
   "source": [
    "FrozenLakeQLearn_EpsilonGreedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, we find that this overly naive approach to Q-Learning results in a lackluster performance of 0.1215 over 10,000 iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi\n",
    "\n",
    "As mentioned, the Taxi Problem includes a larger state space and a longer time horizon between start and goal states, with steady punishment for taking longer, inefficient routes to passengers and destinations. We want to incentivize our algorithms to pursue the “long game”—that is, to see past the upfront penalties for taking lots of steps to eventually reach the +20 points for a successful dropoff—so we want a discount rate (gamma) close to 1.0. If we set gamma close to 0, the algorithm will instead be incentivized to make the agent wander around aimlessly before eventually entering an absorbing state for -10 points, which would be heavily discounted so as not to matter much at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    i = 0\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        if i % 100 == 0:\n",
    "            print('Step: ', i)\n",
    "        i += 1\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    scores = []\n",
    "    for i in range(n):\n",
    "        step, score = count(env, policy, gamma)\n",
    "        steps.append(step)\n",
    "        scores.append(score)\n",
    "    return np.mean(steps), np.mean(scores)\n",
    "\n",
    "# count the average number of steps taken to reach the goal state. \n",
    "def count(env, policy, gamma):\n",
    "    curr_state = env.reset()\n",
    "    counter = 0\n",
    "    reward = None\n",
    "    totalReward = 0\n",
    "    while reward != 20:\n",
    "        state, reward, done, info = env.step(np.argmax(policy[curr_state]))  \n",
    "        totalReward += gamma ** counter * reward\n",
    "        curr_state = state\n",
    "        counter += 1\n",
    "    return counter, totalReward\n",
    "\n",
    "def TaxiVI(g = 0.99, n = 100):\n",
    "    gamma = g\n",
    "\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    start = timer()\n",
    "    policy, V = value_iteration(env, discount_factor=gamma);\n",
    "    end = timer()\n",
    "\n",
    "    policy_steps, policy_score = evaluate_policy(env, policy, gamma, n=n)\n",
    "\n",
    "    print('Policy average score = ', policy_score)\n",
    "    print('Policy average steps = ', policy_steps)\n",
    "    print('Time elapsed: ', end - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Step:  100\n",
      "Step:  200\n",
      "Step:  300\n",
      "Step:  400\n",
      "Step:  500\n",
      "Step:  600\n",
      "Step:  700\n",
      "Step:  800\n",
      "Step:  900\n",
      "Step:  1000\n",
      "Step:  1100\n",
      "Step:  1200\n",
      "Value-iteration converged at iteration# 1217.\n",
      "Policy average score =  6.93708752167\n",
      "Policy average steps =  12.5\n",
      "Time elapsed:  11.448386250063777\n"
     ]
    }
   ],
   "source": [
    "TaxiVI(g=0.99, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of evaluating the average scores of the solution policy, we will increase the margin of error for convergence from 1e-20 to 1e-4 so that the algorithm will converge within a reasonable amount of time. When we run value iteration on gamma = 0.99, it converges in 1217 iterations and 11.79 seconds wall clock time. Evaluating the resultant policy 100 times gives an average 12.5 steps and score of 6.937. Without directly comparing to the results for PI, we can imagine that the solution policy generally takes the taxi through the optimal routes for each of the 20 subproblems, which take average 12.5 steps to complete, or about -13 points. That gives us an average score around 7 from successfully delivering the passenger to the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solving Taxi environment using Policy iteration.\n",
    "\"\"\"\n",
    "\n",
    "env_name  = 'Taxi-v2'\n",
    "env = gym.make(env_name)\n",
    "env = env.unwrapped\n",
    "\n",
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        print('Step: ', i)\n",
    "        i += 1\n",
    "        \n",
    "        # Evaluate the current policy\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        \n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(env.nS):\n",
    "            # The best action we would take under the currect policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = one_step_lookahead(s, V)\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        if policy_stable:\n",
    "            print ('Policy-iteration converged at iteration# %d.' %(i+1))\n",
    "            return policy, V\n",
    "        \n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    scores = []\n",
    "    for i in range(n):\n",
    "        step, score = count(env, policy, gamma)\n",
    "        steps.append(step)\n",
    "        scores.append(score)\n",
    "    return np.mean(steps), np.mean(scores)\n",
    "\n",
    "# count the average number of steps taken to reach the goal state. \n",
    "def count(env, policy, gamma):\n",
    "    curr_state = env.reset()\n",
    "    counter = 0\n",
    "    reward = None\n",
    "    totalReward = 0\n",
    "    while reward != 20:\n",
    "        state, reward, done, info = env.step(np.argmax(policy[curr_state]))  \n",
    "        totalReward += gamma ** counter * reward\n",
    "        curr_state = state\n",
    "        counter += 1\n",
    "    return counter, totalReward\n",
    "\n",
    "def TaxiPI(g = 0.99, n = 100):\n",
    "    gamma = g\n",
    "\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    start = timer()\n",
    "    policy, V = policy_improvement(env, discount_factor=gamma);\n",
    "    end = timer()\n",
    "\n",
    "    policy_steps, policy_score = evaluate_policy(env, policy, gamma, n=n)\n",
    "\n",
    "    print('Policy average score = ', policy_score)\n",
    "    print('Policy average steps = ', policy_steps)\n",
    "    print('Time elapsed: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Step:  1\n",
      "Step:  2\n",
      "Step:  3\n",
      "Step:  4\n",
      "Step:  5\n",
      "Step:  6\n",
      "Step:  7\n",
      "Step:  8\n",
      "Step:  9\n",
      "Policy-iteration converged at iteration# 11.\n",
      "Policy average score =  6.93708752167\n",
      "Policy average steps =  12.5\n",
      "Time elapsed:  67.718011562014\n"
     ]
    }
   ],
   "source": [
    "TaxiPI(g=0.99, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the solution policy from PI is the same, and so has the same average steps and average score as VI. However, policy iteration on gamma = 0.99 converges in 9 iterations, but 67.72 seconds wall clock time—over 5 times as long as VI! This is because policy evaluation implies solving the Bellman equation typically through a system of equations, which is very costly for large state-action sets. Although PI converges faster than VI in many scenarios including Frozen Lake, in this case, the additional computational complexity of the inner loop made PI converge slower than VI in real time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "For the sake of comparison, we will use the same Q-Learning algorithm implementation as we did for Frozen Lake, including the fixed learning rate and exploration strategy of greedily choosing from the Q-table with noise that decays as iterations increase. This algorithm will have a Q-table of 500 (size of state space) × 6 (size of action space) = 3000. We choose to train for 10,000 iterations in order to hypothetically cover every state-action pair plenty of times when graphing performance against learning rate. Because our choice of gamma does not negatively affect time to convergence in Q-Learning, we will choose to use gamma = 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solving Taxi environment using Q-Learning\n",
    "\"\"\"\n",
    "\n",
    "env_name = 'Taxi-v2'\n",
    "env = gym.make(env_name)\n",
    "env = env.unwrapped\n",
    "\n",
    "def TaxiQLearn_NoisyQ(iter_max = 100, t_max = 150, learning_rate = 0.8, g = 1.0):\n",
    "    # Hyperparameters\n",
    "    num_episodes = iter_max\n",
    "    # Set learning parameters\n",
    "    lr = learning_rate\n",
    "    y = g\n",
    "    \n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    #Initialize table with all zeros\n",
    "    Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    #create lists to contain total rewards and steps per episode\n",
    "    jList = []\n",
    "    rList = []\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < t_max:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "    \n",
    "    end = timer()\n",
    "    \n",
    "    print (\"Score over time: \" +  str(sum(rList)/num_episodes))\n",
    "    print (\"Average steps: \" +  str(sum(jList)/num_episodes))\n",
    "    print('Time elapsed: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 5.4705\n",
      "Average steps: 14.3946\n",
      "Time elapsed:  5.054019118892029\n",
      "Score over time: 5.6538\n",
      "Average steps: 14.256\n",
      "Time elapsed:  4.794186649029143\n",
      "Score over time: 5.7583\n",
      "Average steps: 14.1917\n",
      "Time elapsed:  5.51989335892722\n",
      "Score over time: 5.8352\n",
      "Average steps: 14.1736\n",
      "Time elapsed:  5.051946762949228\n",
      "Score over time: 5.9602\n",
      "Average steps: 14.1137\n",
      "Time elapsed:  5.379903438966721\n",
      "Score over time: 5.8457\n",
      "Average steps: 14.1196\n",
      "Time elapsed:  5.5309618200408295\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.6, 0.7, 0.8, 0.9, 0.99, 1.0]:\n",
    "    TaxiQLearn_NoisyQ(iter_max=10000, t_max=300, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average steps for each learning rate are all around 14, but the average reward per episode peaks sharply at learning rate = 0.99. Graphing the performance, we see that the average scores of 10,000 or 100,000 cumulative episodes are between 5.9 and 8.1, asymptotically approaching 12.5. This is expected behavior, and closely mirrors that seen in Q-Learning for Frozen Lake, since the RL algorithm can only approximate the optimal policy based on the values populated in the Q-table by the episodes it has trained on but runs into linear training time constraints. \n",
    "\n",
    "This phenomenon of Q-Learning approaching but not reaching the optimal policy found by VI and PI can also be seen in the average number of steps for each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: -438.0\n",
      "Average steps: 150.0\n",
      "Time elapsed:  0.012774679926224053\n",
      "Score over time: -472.2\n",
      "Average steps: 150.0\n",
      "Time elapsed:  0.07571056205779314\n",
      "Score over time: -424.5\n",
      "Average steps: 150.0\n",
      "Time elapsed:  0.06688580999616534\n",
      "Score over time: -170.03\n",
      "Average steps: 111.08\n",
      "Time elapsed:  0.4407082989346236\n",
      "Score over time: -96.825\n",
      "Average steps: 75.45\n",
      "Time elapsed:  0.5444652970181778\n",
      "Score over time: -39.042\n",
      "Average steps: 40.194\n",
      "Time elapsed:  0.7720109439687803\n",
      "Score over time: -15.84\n",
      "Average steps: 26.655\n",
      "Time elapsed:  0.9311205690028146\n",
      "Score over time: 5.8963\n",
      "Average steps: 14.0852\n",
      "Time elapsed:  5.377058368991129\n",
      "Score over time: 8.09169\n",
      "Average steps: 12.80646\n",
      "Time elapsed:  46.427650494966656\n"
     ]
    }
   ],
   "source": [
    "for iterations in [1, 5, 10, 100, 200, 500, 1000, 10000, 100000]:\n",
    "    TaxiQLearn_NoisyQ(iter_max=iterations, g=1.0, learning_rate=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solving Taxi environment using Q-Learning\n",
    "\"\"\"\n",
    "\n",
    "env_name = 'Taxi-v2'\n",
    "env = gym.make(env_name)\n",
    "env = env.unwrapped\n",
    "\n",
    "def TaxiQLearn_EpsilonGreedy(iter_max = 100, t_max = 150, learning_rate = 0.8, g = 1.0, epsilon = 0.05):\n",
    "    # Hyperparameters\n",
    "    num_episodes = iter_max\n",
    "    # Set learning parameters\n",
    "    lr = learning_rate\n",
    "    y = g\n",
    "    \n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    #Initialize table with all zeros\n",
    "    Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    #create lists to contain total rewards and steps per episode\n",
    "    jList = []\n",
    "    rList = []\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < t_max:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                a = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                a = np.argmax(Q[s,:])\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "    \n",
    "    end = timer()\n",
    "    \n",
    "    print (\"Score over time: \" +  str(sum(rList)/num_episodes))\n",
    "    print (\"Average steps: \" +  str(sum(jList)/num_episodes))\n",
    "    print('Time elapsed: ', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Q-Learning runs for 100,000 episodes, the cumulative number of steps averages to 12.81 per episode, quite close to the 12.5 steps in VI and PI. Finally, if we contrast with a simple epsilon-greedy approach to selecting actions in the Q-Learning algorithm where epsilon stays fixed at 0.05, we see that the performance is worse for the same reasons as in Frozen Lake—the algorithm is unable to take advantage of its training time to more fully explore the state-action space early on, nor is it able to fully exploit the information it has gained later on. It results in an average score of only 4.914, contrasted with 8.092 seen in the above greedy noisy Q-Learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 4.91436\n",
      "Average steps: 14.10828\n",
      "Time elapsed:  35.80320289905649\n"
     ]
    }
   ],
   "source": [
    "TaxiQLearn_EpsilonGreedy(iter_max=100000, g=1.0, learning_rate=0.99)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
